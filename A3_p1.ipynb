{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up text as list of lists of string tokens.\n",
    "with open(\"15pctmasked.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "        sentences.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up bigram model as dict of dicts mapping to log probabilities, as well as vocabulary.\n",
    "with open(\"lm.txt\") as f:\n",
    "    model = {}\n",
    "    vocab = {}\n",
    "    vocab_list = []\n",
    "    index = 0\n",
    "    for line in f.readlines():\n",
    "        components = line.split()\n",
    "        if components[0] not in model:\n",
    "            model[components[0]] = {}\n",
    "        model[components[0]][components[1]] = np.log(float(components[2]))\n",
    "        if components[0] not in vocab:\n",
    "            vocab[components[0]] = index\n",
    "            vocab_list.append(components[0])\n",
    "            index += 1\n",
    "            \n",
    "    # Fill out unseen words.\n",
    "    for key1 in model:\n",
    "        for key2 in vocab:\n",
    "            if key2 not in model[key1]:\n",
    "                model[key1][key2] = -np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK = \"<mask>\"\n",
    "START = \"<start>\"\n",
    "STOP = \"<eos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_score_table(sentence, model, vocab):\n",
    "    seq_len = len(sentence)\n",
    "    vocab_size = len(vocab)\n",
    "    s = np.zeros((seq_len - 1, vocab_size, vocab_size))\n",
    "    \n",
    "    for i in range(seq_len - 1):\n",
    "        if sentence[i + 1] == MASK:\n",
    "            for prev_key in vocab:\n",
    "                for key in vocab:\n",
    "                    s[i, vocab[prev_key], vocab[key]] = model[prev_key][key]\n",
    "        else:\n",
    "            for prev_key in vocab:\n",
    "                for key in vocab:\n",
    "                    s[i, vocab[prev_key], vocab[key]] = 0 if key == sentence[i + 1] else -np.inf\n",
    "                    \n",
    "    return s\n",
    "            \n",
    "\n",
    "def decode(sentence, model, vocab, vocab_list):\n",
    "    \"\"\"\n",
    "    Decode '<mask>' tokens using Viterbi algorithm.\n",
    "    \"\"\"\n",
    "    seq_len = len(sentence)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    decoded = []\n",
    "    \n",
    "    # Viterbi variables.\n",
    "    v = np.zeros((seq_len - 2, vocab_size))\n",
    "    b = np.zeros((seq_len - 2, vocab_size), dtype = int)\n",
    "    s = make_score_table(sentence, model, vocab)\n",
    "    \n",
    "    v[0, :] = s[0, vocab[START], :]\n",
    "        \n",
    "    for i in range(1, seq_len - 2):\n",
    "        v[i, :] = np.max(s[i, :, :] + np.expand_dims(v[i - 1, :], 1), axis = 0)\n",
    "        b[i, :] = np.argmax(s[i, :, :] + np.expand_dims(v[i - 1, :], 1), axis = 0)\n",
    "        \n",
    "    # Append last word.\n",
    "    decoded.append(STOP)\n",
    "    word = vocab_list[np.argmax(s[seq_len - 2, :, vocab[STOP]] + v[seq_len - 3, :])]\n",
    "    decoded.append(word)\n",
    "    for i in range(seq_len - 3, 0, -1):\n",
    "        word = vocab_list[b[i, vocab[word]]]\n",
    "        decoded.append(word)\n",
    "    decoded.append(START)\n",
    "    decoded.reverse()\n",
    "    \n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'I', '<s>', 'p', 'e', '<s>', 'm', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', '<s>', 'o', 'f', '<s>', 'G', 'e', 'o', 'r', 'g', 'i', 'a', \"'\", '<s>', '<s>', 'a', 'u', 'r', 'o', 'm', 'o', 'b', 'i', 'l', 'e', '<s>', 't', 'i', 't', 'l', 'e', '<s>', 'l', 'a', 'w', '<s>', 'w', 'a', 's', '<s>', 'a', 'l', 'y', '<s>', '<s>', 't', 'e', 'c', 'o', 'm', 'm', 'e', 'n', 'd', 'e', 'd', '<s>', 'b', 'e', '<s>', 't', 'h', 'e', '<s>', 'o', 'u', 't', 'g', 'o', 'i', 'n', 'g', '<s>', 'j', 'u', 'r', 'y', '<s>', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "decoded = decode(sentences[0], model, vocab, vocab_list)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sentence 0 ...\n",
      "Writing sentence 100 ...\n",
      "Writing sentence 200 ...\n",
      "Writing sentence 300 ...\n",
      "Writing sentence 400 ...\n",
      "Writing sentence 500 ...\n",
      "Writing sentence 600 ...\n",
      "Writing sentence 700 ...\n",
      "Writing sentence 800 ...\n",
      "Writing sentence 900 ...\n",
      "Writing sentence 1000 ...\n",
      "Writing sentence 1100 ...\n",
      "Writing sentence 1200 ...\n",
      "Writing sentence 1300 ...\n",
      "Writing sentence 1400 ...\n",
      "Writing sentence 1500 ...\n",
      "Writing sentence 1600 ...\n",
      "Writing sentence 1700 ...\n",
      "Writing sentence 1800 ...\n",
      "Writing sentence 1900 ...\n",
      "Writing sentence 2000 ...\n",
      "Writing sentence 2100 ...\n",
      "Writing sentence 2200 ...\n",
      "Writing sentence 2300 ...\n",
      "Writing sentence 2400 ...\n",
      "Writing sentence 2500 ...\n",
      "Writing sentence 2600 ...\n",
      "Writing sentence 2700 ...\n",
      "Writing sentence 2800 ...\n",
      "Writing sentence 2900 ...\n",
      "Writing sentence 3000 ...\n",
      "Writing sentence 3100 ...\n",
      "Writing sentence 3200 ...\n",
      "Writing sentence 3300 ...\n",
      "Writing sentence 3400 ...\n",
      "Writing sentence 3500 ...\n",
      "Writing sentence 3600 ...\n",
      "Writing sentence 3700 ...\n",
      "Writing sentence 3800 ...\n",
      "Writing sentence 3900 ...\n",
      "Writing sentence 4000 ...\n",
      "Writing sentence 4100 ...\n",
      "Writing sentence 4200 ...\n",
      "Writing sentence 4300 ...\n",
      "Writing sentence 4400 ...\n",
      "Writing sentence 4500 ...\n",
      "Writing sentence 4600 ...\n",
      "Writing sentence 4700 ...\n",
      "Writing sentence 4800 ...\n",
      "Writing sentence 4900 ...\n",
      "Writing sentence 5000 ...\n",
      "Writing sentence 5100 ...\n",
      "Writing sentence 5200 ...\n",
      "Writing sentence 5300 ...\n",
      "Writing sentence 5400 ...\n",
      "Writing sentence 5500 ...\n",
      "Writing sentence 5600 ...\n",
      "Writing sentence 5700 ...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(\"output.txt\", \"w\") as f:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            if i % 100 == 0:\n",
    "                print(\"Writing sentence %d ...\" % i)\n",
    "            decoded = decode(sentence, model, vocab, vocab_list)\n",
    "            f.write(\"%s\\n\" % (\" \".join(decoded)))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Graceful Exit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
